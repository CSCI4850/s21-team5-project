{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% PACKAGES INCLUDED HERE \n",
    "% DO NOT NEED TO CHANGE\n",
    "\\documentclass[conference]{IEEEtran}\n",
    "%\\IEEEoverridecommandlockouts\n",
    "% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.\n",
    "\\usepackage{cite}\n",
    "\\usepackage{amsmath,amssymb,amsfonts}\n",
    "\\usepackage{algorithmic}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{textcomp}\n",
    "\\def\\BibTeX{{\\rm B\\kern-.05em{\\sc i\\kern-.025em b}\\kern-.08em\n",
    "    T\\kern-.1667em\\lower.7ex\\hbox{E}\\kern-.125emX}}\n",
    "\\begin{document}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% TITLE GOES HERE\n",
    "\n",
    "\\title{Sorting Aid: Document Classification\\\\}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% AUTHOR NAMES GOES HERE\n",
    "\\author{\n",
    "\\IEEEauthorblockN{Emily Turner}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "ert3c@mtmail.mtsu.edu}\\\\\n",
    "\\IEEEauthorblockN{Tyler Christian}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "tkc2s@mtmail.mtsu.edu}\\\\\n",
    "\\and\n",
    "\\IEEEauthorblockN{Dylan Fox}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "djf3f@mtmail.mtsu.edu}\\\\\n",
    "\\IEEEauthorblockN{Munayfah Albaqami}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "maa9e@mtmail.mtsu.edu}\\\\\n",
    "\\and\n",
    "\\IEEEauthorblockN{Anthony Ghebranious}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "ang5v@mtmail.mtsu.edu}\\\\\n",
    "}\n",
    "\\maketitle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% ABSTRACT \n",
    "\n",
    "\\begin{abstract}\n",
    "Information that best describes galaxy mergers is sparse.  Complications arise when forming postulations due to missing information such as orbital mass-ratios, orbital velocities, and orientations.  To remedy this, we devised two neural networks to analyze images and human scores from Galaxy Zoo: Mergers in the hopes of finding a well-fitting model, one that can find true dynamic parameters for galaxy collisions.  In the single layer neural network, the information is parsed to inform the convolution net about hyperparameters such as batch or epoch sizes.  The convolution model is designed to assign a fitness score to an input galaxy image.  That score represents an approximation of how well the image fits into predicted galaxy collision simulations.  Through the limited scope of this project there are points of interest.  Both networks achieved successful validation while eventually overfitting.  The convolution net managed with good accuracy to predict scores of mergers when compared to human grading and, perhaps, more accurate than the human scores. Though the data set we used is small and unevenly distributed, there are hopeful interpretations of the results.  Better and more data sets along with more network layers may yield better results in the future.  With more tuning, our neural networks may eliminate the need for further human rankings\n",
    "\n",
    "^DELETE WHEN DONE^\n",
    "\n",
    "Need more about what exactly was used, how, and what it did (cant write about this yet)\n",
    "Time is of the essence in all workplaces. Tasks should be peformed optimally to ensure that the most amount of work is completed in the least amount of allocated time; however, complications arise when a task is tedious, complex, or intricate with details. As a result, productivity is lost and less work is completed. This situation can be especially common when sorting documents. Undergoing a task such as sorting documents requires classification, efficiency, and, most importantly, accuracy. For humans, this task becomes increasingly difficult over time, which can lead to sorting mistakes or allocating too much time to sorting when there are tools to provide significant aid. One such tool is the use of Convolutional Neural Networks (CNN). From using this too, an effective evaluation of classifying documents while simultaneously achieving validation without overfitting data was achieved. The network built was able to identify and sort document from eight variants despite implementing a small data set. With further implementation, this work would be able to be interfaceable with other systems, simplifying work. \n",
    "\\end{abstract}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% KEYWORDS\n",
    "\n",
    "\\begin{IEEEkeywords}\n",
    "convolutional neural networks, CNN, document classification, document sorting \n",
    "\\end{IEEEkeywords}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% INTRODUCTION SECTION\n",
    "\\section{Introduction}\n",
    "\n",
    "Everyone has documents whether they are invoices or records. Sometimes keeping track of those documents can be tough. On top of that, every document has many different elements on it. Often times we just need to find something specific on a document and we can't find the document. Or the document is long and we aren't able to find it. Often times it is very hard to organize all the data from those documents resulting in important information not being found or some of the invoices not being paid. \n",
    "\n",
    "Much of the data being used is from real documents. By using real documents we can ensure that the neural net performs the task on data that a user might actually have. \n",
    "\n",
    "This project extracts the data from all the documents in a data set. After extracting all the data it then categorizes it and sends it to a spreadsheet for easier viewing. The goal is to perform this task using a neural net that can do all this on its own with little to no human aid. Having a tool like this it will make it much easier to pay all invoices on time or finding the information the moment you need it. The neural net will allow you to see all your different documents on a single document instead of searching through the many documents that you have. This neural net will organize everything in a spreadsheet which allows the user to know exactly where their money is going or know important details without looking through every one of your documents. When looking at invoices in particular it makes it easier for them to cut out reoccurring payments that may be unnecessary when on a tight budget."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% BACKGROUND SECTION\n",
    "\\section{Background}\n",
    "\n",
    "One motivation of our work was to create a product that would be applicable to other systems, simplifying work, Our aim is to create one CNN that will take in various document images (with noise) and classify that document based on pixel intensities. If multiple images have pixel intensities in the same areas, then those documents most likely belong together. While such a goal would yield practical, diverse results, this means that challenges are also practical and diverse. For example, even after creating categories for documents, each category will still have great variability. One document could even appear as another (an email could look like a memo, or an invoice could look like a receipt). This would mean that generalization is a core challenge in this project.\n",
    "\n",
    "Such ambitions and challenges were experienced by Harley, Ufkes, and Derpanis when also sorting documents, who also used CNNS to sort documents. The main difference, however, is that our work only uses one CNN while Harley, Ufkes, and Derpanis use two CNNs, one as a small, holistic CNN while the other is a “container” CNN, which held the smaller CNN [citation]. Our CNN acts as an all-purpose CNN, with [Emily or Dylan should finish this off explaining what our CNN has that is different from this other work].\n",
    "\n",
    "Another similar work includes the work of Cheng et al., where a CNN was also used to classify documents. However, the process of doing so was different. Instead of using images of various documents, textual relationships in documents are stored by additionally using a “long short-term memory recurrent network to obtain the high-level abstract representation[s]” of text throughout a document [citation here, -2nd source-]. Our network is different in that we do not use Natural Language Processing (NLP) to feed data into our CNN since we are using images. This would mean that preserving spatial relationships are not as important in Cheng et al.’s work since sequential relationships are more important. In addition, intentionally feeding noise into their CNN could mitigate progress since the CNN must rely on not editing data while our network must rely on intentionally creating noise so we can pass the most amount of the most various document images into our network.\n",
    "\n",
    "Last, Muhammad Zeshan Afzal also created a similar work using a method that is closely related to our method. They used a database of approximately 1.2 million RGB images. A major difference between our methods is Afzal downsizes the images to 227 x 227 in order to \"lower the computational complexity of the system\" [Citation (3rd Source)]. Another difference between the two CNN is our CNN doesn't focus so much on the channel of the images, NAME HERE converts any grey samples in the data set to a three-channel that way it can conform to an RGB format.\n",
    "\n",
    "\n",
    "\n",
    " \\cite{WallinJSPAM}\\cite{WallinGalaxyZoo}.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% METHODS SECTION\n",
    "\\section{Methods}\n",
    "\n",
    "\\subsection{Data}\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/distribution.png}}\n",
    "\\caption{Distribution of sample models across a range of human scores from 0-1.}\n",
    "\\label{fig:distribution}\n",
    "\\end{figure}\n",
    "\n",
    "We chose one of the 62 pairs of galaxies and pulled the associated data. This set consisted of 1294 grayscale model images in .png format and their associated human scores in a text file. The set was already sorted in descending order by score. However, we realized that this data was heavily skewed towards worse models, with more than half of the models having scores below 0.10. In an effort to combat overfitting on bad model images, we used only the first 600 images, which, while still skewed towards worse models (see Fig. \\ref{fig:distribution}), presented a much more evenly distributed data set.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/target.png}}\n",
    "\\caption{The chosen colliding galaxy pair for this project. A higher model score should represent a model that closely resembles this image.}\n",
    "\\label{fig:TargetGalaxy}\n",
    "\\end{figure}\n",
    "\n",
    "The images and scores were read in using a technique similar to the MNIST problem presented in OLA 6 \\cite{phillips}. The main difference being that the grab\\_image function was modified to read in our images as greyscale rather than full RGB and at a reduced resoultion of 100x100. We then shuffled the images, taking care to manipulate them in such a way that the scores would retain the same index as their respective images. This shuffling ensured that models of similar scores would be randomly distributed, preventing the model from learning soley on one quality of model at a time. \n",
    "\n",
    "The 600 shuffled images were then split into a training and validation set consisting of 510 images and a testing set with the remaining 90 images. When training our nets, a validation split of 0.3 was used, resulting in the nets training on 357 images and validating on the remaining 153.\n",
    "\n",
    "\\subsection{Single Layer Neural Net}\n",
    "\n",
    "For the single layer net, the images were flattened to 1D arrays then passed into a single dense layer. This layer was given an input size of 1 and a Softmax activation function in order to return a single score between 0 and 1. This net, and the nets discussed below, were all compiled with mean squared error, which is the best fit for regression problems, as the loss parameter.\n",
    "\n",
    "In training our nets, we found that a small batch size was an absolute necessity for decent learning, most likely due to the small data set and skew towards worse models. We used a batch size of 4 for all of the nets discussed in this project.\n",
    "\n",
    "\\subsection{Convolutional Neural Net}\n",
    "\n",
    "In our second net, we attempted to utilize the superior image recognition characteristics of a convolutional neural net. As a base, we used the convolutional neural net used on the cat problem in OLA 6 \\cite{phillips}. In this net, there are two 2D convolution layers that feed into a pooling layer.  The main changes made were to kernel size and the final dense layer. There are two 2D convolution layers that use a standard ReLu activation function, which are then followed by pooling and dropout layer. It is then passed into a flattening layer and a single dense layer, still with a ReLu activation function, before passing though a final dropout layer and into the final dense layer. The main changes we made to this architecture from OLA 6 were in terms of kernel size, unit size of the dense and convolution layers, and the setup of the final dense layer. \n",
    "\n",
    "We adjusted the kernel sizes to be slightly larger. This was done in an attempt to better capture what we believed were the most important characteristics of the image, namely the tidal distortions around the edges of each galaxy and the bridge, or lack thereof, between the galaxies. The unit sizes in the convolution layers were made smaller in order to fit our relatively small data set. Without doing this, we experienced drastic overfitting or no training at all. The final dense layer was also changed to match our plan for a regression model. The input for the final layer was reduced to 1 and the activation changed to Softmax in order to return a single score between 0 and 1 for the model image.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.5\\linewidth]{./Images/convNet.png}}\n",
    "\\caption{Layout of the convolutional neural net}\n",
    "\\label{fig:ConvNetArchitecture}\n",
    "\\end{figure}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% RESULTS SECTION\n",
    "\\section{Results}\n",
    "\n",
    "\\subsection{Single Layer Net Results}\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/SingleModelLoss.png}}\n",
    "\\caption{Loss (Mean Squared Error) for the single layer net over 120 epochs}\n",
    "\\label{fig:SingleModelLoss}\n",
    "\\end{figure}\n",
    "\n",
    "Shown in Fig. \\ref{fig:SingleModelLoss} is the training and validation loss history over 120 epochs. At the end of 120 epochs, the model had decreased to a MSE loss of 0.012 for the training data and 0.019 for the validation data. We can also plainly see the overfitting that plagued both of our neural nets, regardless of the hyperparameter tuning we performed to try to remove it.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/SinglePredictedVAct.png}}\n",
    "\\caption{Predicted values generated by single layer net vs actual value}\n",
    "\\label{fig:SinglePredictedVAct}\n",
    "\\end{figure}\n",
    "\n",
    "Fig. \\ref{fig:SinglePredictedVAct} shows the values predicted by the single layer net on the 90 testing images verus their generated scores. We see a linear correlation between the two, which is what we hoped to observe. A perfect linear correlation would have meant that the model was guessing exactly the same as the human score.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/SingleDiffVAct.png}}\n",
    "\\caption{Difference between predicted values generated by single layer net and actual values vs actual value}\n",
    "\\label{fig:SingleDiffVAct}\n",
    "\\end{figure}\n",
    "\n",
    "Fig. \\ref{fig:SingleDiffVAct} shows the difference between the values predicted by the single layer net for the 90 test images and their human scores versus the human scores. From this plot, it is easy to see that the net had a tendency to overscore models with low human scores and underscore models with high human scores. We can also see that the most accurate predictions occured for models with lower human scores, which makes sense given the data's skew towards worse models. Over the full 90 image set, the net attained a mean difference of 0.093 between its predicted values and the human scores.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/singleWeights.png}}\n",
    "\\caption{Visualization of weights for the single layer net}\n",
    "\\label{fig:singleWeights}\n",
    "\\end{figure}\n",
    "\n",
    "The final weights for the single layer net are shown in Fig. \\ref{fig:singleWeights}. We see that the net is placing emphasis on the bridge forming between the two galaxies as well as on the tidal distortions around the edges. Interestingly, the net also thinks that the core of the left galaxy is extremely important. This may be due to the fact that all of our model images are positioned roughly the same way, and in every image there is a bright spot marking the center of a galaxy at roughly the hotspot location shown in Fig. \\ref{fig:singleWeights}.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/SinglePredictedBest.png}}\n",
    "\\caption{Model that was scored the highest by the single layer neural net}\n",
    "\\label{fig:SinglePredictedBest}\n",
    "\\end{figure}\n",
    "\n",
    "\n",
    "Shown in Fig. \\ref{fig:SinglePredictedBest} is the model that the single layer net scored the highest (0.846). Its real human score was 0.946, so this shows good agreement between the net's perception and human perception of what the best model is. We can also visually verify that this chosen model closely resembles the target image shown in Fig. \\ref{fig:TargetGalaxy}.\n",
    "\n",
    "\\subsection{Convolutional Neural Net Results}\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/ConvModelLoss.png}}\n",
    "\\caption{Layout of the convolutional neural net}\n",
    "\\label{fig:ConvModelLoss}\n",
    "\\end{figure}\n",
    "\n",
    "In Fig. \\ref{fig:ConvModelLoss} we see very similar learning behavior to the single layer net. Both display overfitting issues and, after 120 epochs, reach approximately the same MSE. For the convolutional net, the final MSE for the training data was 0.013 and 0.021 which is just slightly worse than the single layer net. Qualitatively, the only difference in learning behavior was the prescence of a slight plateau for the fist 10 or so epochs before the net actually began to learn quickly.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/ConvPredictedVAct.png}}\n",
    "\\caption{Predicted values generated by convolutional net vs actual value}\n",
    "\\label{fig:ConvPredictedVAct}\n",
    "\\end{figure}\n",
    "\n",
    "Like with the single layer net, Fig. \\ref{fig:ConvPredictedVAct} shows a linear correlation between the predicted values and human scores. However, as the human scores increase, we see a wider spread in predicted values when compared to the single layer net. \n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/ConvDiffVAct.png}}\n",
    "\\caption{Difference between predicted values generated by convolutional net and actual values vs actual value}\n",
    "\\label{fig:ConvDiffVAct}\n",
    "\\end{figure}\n",
    "\n",
    "Once again, Fig. \\ref{fig:ConvDiffVAct} shows that the convolutional net has a tendency to overscore worse models and underscore better models. However, when compared to the single layer net, we see a much tighter grouping of worse models (human scores $\\leq0.3$) that the model was able to accurately guess with a difference of $-0.1 \\leq$ Diff $\\leq 0.1$. For the 90 image testing set, the convoltional net attained a mean difference of 0.112, which is slightly worse than the single layer net. \n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/convWeight1.png}}\n",
    "\\caption{Visualization of weights for the first convolution layer}\n",
    "\\label{fig:convWeight1}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/convWeight2.png}}\n",
    "\\caption{Visualization of weights for the second convolution layer}\n",
    "\\label{fig:convWeight2}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/layer1activations.png}}\n",
    "\\caption{Visualization of network activations for the first convolution layer}\n",
    "\\label{fig:layer1activations}\n",
    "\\end{figure}\n",
    "\n",
    "Fig. \\ref{fig:convWeight1} and Fig. \\ref{fig:convWeight2} show the weights for the first and second convolution layers respectively. However, it is hard to glean any useful information from just these weights on their own. That is why we have shown the activations of units in the first layer in Fig. \\ref{fig:layer1activations}. We can see that for this specific activation, the net was processing a model that was very similar to the target image. We can see units that are picking up the major features: galactic cores, tidal distortions, and the bridge. We can also plainly see the units that are detecting the edges of the galaxies, as well as some units that aren't really being utilized.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/ConvPredictedBest.png}}\n",
    "\\caption{Model that was scored the highest by the convolutional neural net}\n",
    "\\label{fig:ConvPredictedBest}\n",
    "\\end{figure}\n",
    "\n",
    "Fig. \\ref{fig:ConvPredictedBest} shows the image that was given the highest score by the convolutional net (0.695). This predicted value is significantly lower than the human score of 0.853, but it shows that the  convolutional net was still atleast somewhat successful in deciding on what the best model should look like. Once again, we can visually verify that the convolutional net's predicted best model closely resembles the target image shown in Fig. \\ref{fig:TargetGalaxy}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% DISCUSSION SECTION\n",
    "\\section{Discussion}\n",
    "\n",
    "Both the single layer neural network and convolutional neural network show promise in correctly predicting how good of a fit a model is for a target galaxy collision. Both nets achieved a MSE of $\\approx 0.02$ on the validation data and a mean difference of $\\approx 0.1$ when tasked with predicting the scores of 90 test models. Both nets successfully chose an image out of a set of 90 that they decided was the best model for the target image. Upon visual comparison, these two chosen models very closely resemble the target galaxy collision. However, both models struggled with overfitting which can most likely be attributed to both our data set's small size and its uneven distribution of model quality. \n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/ConvHumanNoiseExample.png}}\n",
    "\\caption{Example of a model that the convolutional net scored \"poorly\" when just comparing against human score.}\n",
    "\\label{fig:ConvHumanNoiseExample}\n",
    "\\end{figure}\n",
    "\n",
    "Another factor to consider is the noise associated with the human scores. As a citizen scientist project, the models in the Galaxy Zoo: Mergers data set were ranked by people with minimal training. There are also no checks in place to prevent against people clicking though models without consideration, artificially inflating or deflating their scores. We must also acknowledge the randomness of the tournament system. It is entirely possible that some models were \"unlucky\" and went against rounds of exclusively good models, causing them to be scored lower than they actually should have or vice versa.\n",
    "\n",
    "Take for example the model shown in Fig. \\ref{fig:ConvHumanNoiseExample}. This was one of the convolutional net's worst predictions, with a difference of nearly 0.5 between the net's predicted score of 0.195 and the human score of 0.676. However, if we visually compare this model with the target image, one could argue that the model did not deserve such a high human score. Instead, the model might better represent a score between the net's prediction and the human score. \n",
    "\n",
    "Future work could pertain to the expansion of the convolutional net to include more layers. Training with these extra layers and units would necessitate a larger data set, hopefully with a more even distribution of model quality. We believe that such a net would obtain better results than either of the nets described in this paper.  Also, the nets we created for this paper could be applied to the other 61 galaxy pairs, and, with enough training, possibly eliminate the need for further human rankings. This would allow for new models to be created and scored much faster than they have been previously. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% REFERENCES\n",
    "% THIS IS CREATED AUTOMATICALLY\n",
    "\\bibliographystyle{IEEEtran}\n",
    "\\bibliography{references} % change if another name is used for References file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\end{document}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
