{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% PACKAGES INCLUDED HERE \n",
    "% DO NOT NEED TO CHANGE\n",
    "\\documentclass[conference]{IEEEtran}\n",
    "%\\IEEEoverridecommandlockouts\n",
    "% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.\n",
    "\\usepackage{cite}\n",
    "\\usepackage{amsmath,amssymb,amsfonts}\n",
    "\\usepackage{algorithmic}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{textcomp}\n",
    "\\def\\BibTeX{{\\rm B\\kern-.05em{\\sc i\\kern-.025em b}\\kern-.08em\n",
    "    T\\kern-.1667em\\lower.7ex\\hbox{E}\\kern-.125emX}}\n",
    "\\begin{document}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% TITLE GOES HERE\n",
    "\n",
    "\\title{Sorting Aid: Document Classification\\\\}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% AUTHOR NAMES GOES HERE\n",
    "\\author{\n",
    "\\IEEEauthorblockN{Emily Turner}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "ert3c@mtmail.mtsu.edu}\\\\\n",
    "\\IEEEauthorblockN{Tyler Christian}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "tkc2s@mtmail.mtsu.edu}\\\\\n",
    "\\and\n",
    "\\IEEEauthorblockN{Dylan Fox}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "djf3f@mtmail.mtsu.edu}\\\\\n",
    "\\IEEEauthorblockN{Munayfah Albaqami}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "maa9e@mtmail.mtsu.edu}\\\\\n",
    "\\and\n",
    "\\IEEEauthorblockN{Anthony Ghebranious}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "ang5v@mtmail.mtsu.edu}\\\\\n",
    "}\n",
    "\\maketitle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% ABSTRACT \n",
    "\n",
    "\\begin{abstract}\n",
    "Information that best describes galaxy mergers is sparse.  Complications arise when forming postulations due to missing information such as orbital mass-ratios, orbital velocities, and orientations.  To remedy this, we devised two neural networks to analyze images and human scores from Galaxy Zoo: Mergers in the hopes of finding a well-fitting model, one that can find true dynamic parameters for galaxy collisions.  In the single layer neural network, the information is parsed to inform the convolution net about hyperparameters such as batch or epoch sizes.  The convolution model is designed to assign a fitness score to an input galaxy image.  That score represents an approximation of how well the image fits into predicted galaxy collision simulations.  Through the limited scope of this project there are points of interest.  Both networks achieved successful validation while eventually overfitting.  The convolution net managed with good accuracy to predict scores of mergers when compared to human grading and, perhaps, more accurate than the human scores. Though the data set we used is small and unevenly distributed, there are hopeful interpretations of the results.  Better and more data sets along with more network layers may yield better results in the future.  With more tuning, our neural networks may eliminate the need for further human rankings\n",
    "\\end{abstract}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% KEYWORDS\n",
    "\n",
    "\\begin{IEEEkeywords}\n",
    "convolutional neural networks, single layer neural networks, CNN, galaxy collisions, image scoring, document classification, document sorting\n",
    "\\end{IEEEkeywords}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% INTRODUCTION SECTION\n",
    "\\section{Introduction}\n",
    "\n",
    "Everyone has bills that they must pay. Invoices are \n",
    "sent to our emails or in the mail almost daily. Invoices \n",
    "have multiple components on them, including the time stamp \n",
    "when the invoice was created and the date the invoice must \n",
    "be paid. Other components include the cost of the different items \n",
    "that were purchased or simply the cost of the service that was performed. \n",
    "Often times it is very hard to organize all the data from the bills resulting \n",
    "in some of the bills not being paid in time.\n",
    "\n",
    "Much of the data being used is from real invoices that have been \n",
    "given to people. By using real invoices we can ensure that the neural \n",
    "net performs the task on data that a user might have. \n",
    "\n",
    "This project extracts the data from all the invoices in a data set. After \n",
    "extracting all the data it then categorizes it and sends it to a spreadsheet \n",
    "for easier viewing. The goal is to perform this task using a neural \n",
    "net that can do all this on its own with little to no human aid. \n",
    "Having a tool like this it will make it much easier to pay all invoices \n",
    "on time. The neural net will make it easier to see every invoice on a \n",
    "single document instead of searching through the many documents. This \n",
    "neural net will organize everything in a spreadsheet which allows the \n",
    "user to know exactly where their money is going, thereafter making it easier\n",
    "for them to cut out reoccurring payments that may be unnecessary when on a tight budget. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% BACKGROUND SECTION\n",
    "\\section{Background}\n",
    "\n",
    "Our project is based on several pieces of research. First is\n",
    "the work of Toomre and Toomre who demonstrated merging\n",
    "galaxies would cause distinct tidal distortions \\cite{Toomre}. These tidal\n",
    "distortions often result in the galaxies forming “bridges” and\n",
    "“tails”, star regions that appear to bridge the gap between the\n",
    "two galaxies, and arc away from each other. Toomre and\n",
    "Toomre performed a variety of simulations showing that the\n",
    "gravitation interactions caused these distinct shapes \\cite{Toomre}.\n",
    "\n",
    "Second is the work of SPAM. SPAM is O(n) public release\n",
    "software that builds upon the restricted three-body model to\n",
    "simulate galactic models merging. SPAM takes galactic\n",
    "models and simulates the two galaxies merger. In addition,\n",
    "point particles are added, undergoing realistic tidal distortions\n",
    "and produce the iconic bridge and tail shapes \\cite{WallinJSPAM}.\n",
    "\n",
    "Lastly, most of our data has origins from Galaxy Zoo:\n",
    "Mergers. Galaxy Zoo: Mergers is a citizen scientist effort to\n",
    "explore the unknown parameter space for the galactic models.\n",
    "Using the quick efficiency of SPAM to visual the final shape of\n",
    "galactic models, millions of galactic models were simulated\n",
    "and views by thousands of volunteers. Citizen scientists would\n",
    "select models with a similar shape to the target image the\n",
    "models are attempting to represent. From there, those\n",
    "selected models went through a tournament like competition\n",
    "with other models to receive the human fitness score. The\n",
    "citizen scientist would choose the galactic model whose shape\n",
    "better matches the target images. After going through the\n",
    "tournament, all the models would have an associated fitness\n",
    "scores \\cite{WallinGalaxyZoo}.\n",
    "\n",
    "The galactic models and their associated human fitness\n",
    "scores are what were used in this project. The human fitness\n",
    "scores were directly applied, while the galactic models had to\n",
    "be visualized again. The galactic models were sent through\n",
    "SPAM again with a larger number of particles to obtain a \n",
    "higher resolution image of the colliding galaxies. These model\n",
    "images and the human fitness scores are used \\cite{WallinJSPAM}\\cite{WallinGalaxyZoo}.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% METHODS SECTION\n",
    "\\section{Methods}\n",
    "\n",
    "\\subsection{Data}\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/distribution.png}}\n",
    "\\caption{Distribution of sample models across a range of human scores from 0-1.}\n",
    "\\label{fig:distribution}\n",
    "\\end{figure}\n",
    "\n",
    "We chose one of the 62 pairs of galaxies and pulled the associated data. This set consisted of 1294 grayscale model images in .png format and their associated human scores in a text file. The set was already sorted in descending order by score. However, we realized that this data was heavily skewed towards worse models, with more than half of the models having scores below 0.10. In an effort to combat overfitting on bad model images, we used only the first 600 images, which, while still skewed towards worse models (see Fig. \\ref{fig:distribution}), presented a much more evenly distributed data set.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/target.png}}\n",
    "\\caption{The chosen colliding galaxy pair for this project. A higher model score should represent a model that closely resembles this image.}\n",
    "\\label{fig:TargetGalaxy}\n",
    "\\end{figure}\n",
    "\n",
    "The images and scores were read in using a technique similar to the MNIST problem presented in OLA 6 \\cite{phillips}. The main difference being that the grab\\_image function was modified to read in our images as greyscale rather than full RGB and at a reduced resoultion of 100x100. We then shuffled the images, taking care to manipulate them in such a way that the scores would retain the same index as their respective images. This shuffling ensured that models of similar scores would be randomly distributed, preventing the model from learning soley on one quality of model at a time. \n",
    "\n",
    "The 600 shuffled images were then split into a training and validation set consisting of 510 images and a testing set with the remaining 90 images. When training our nets, a validation split of 0.3 was used, resulting in the nets training on 357 images and validating on the remaining 153.\n",
    "\n",
    "\\subsection{Single Layer Neural Net}\n",
    "\n",
    "For the single layer net, the images were flattened to 1D arrays then passed into a single dense layer. This layer was given an input size of 1 and a Softmax activation function in order to return a single score between 0 and 1. This net, and the nets discussed below, were all compiled with mean squared error, which is the best fit for regression problems, as the loss parameter.\n",
    "\n",
    "In training our nets, we found that a small batch size was an absolute necessity for decent learning, most likely due to the small data set and skew towards worse models. We used a batch size of 4 for all of the nets discussed in this project.\n",
    "\n",
    "\\subsection{Convolutional Neural Net}\n",
    "\n",
    "In our second net, we attempted to utilize the superior image recognition characteristics of a convolutional neural net. As a base, we used the convolutional neural net used on the cat problem in OLA 6 \\cite{phillips}. In this net, there are two 2D convolution layers that feed into a pooling layer.  The main changes made were to kernel size and the final dense layer. There are two 2D convolution layers that use a standard ReLu activation function, which are then followed by pooling and dropout layer. It is then passed into a flattening layer and a single dense layer, still with a ReLu activation function, before passing though a final dropout layer and into the final dense layer. The main changes we made to this architecture from OLA 6 were in terms of kernel size, unit size of the dense and convolution layers, and the setup of the final dense layer. \n",
    "\n",
    "We adjusted the kernel sizes to be slightly larger. This was done in an attempt to better capture what we believed were the most important characteristics of the image, namely the tidal distortions around the edges of each galaxy and the bridge, or lack thereof, between the galaxies. The unit sizes in the convolution layers were made smaller in order to fit our relatively small data set. Without doing this, we experienced drastic overfitting or no training at all. The final dense layer was also changed to match our plan for a regression model. The input for the final layer was reduced to 1 and the activation changed to Softmax in order to return a single score between 0 and 1 for the model image.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.5\\linewidth]{./Images/convNet.png}}\n",
    "\\caption{Layout of the convolutional neural net}\n",
    "\\label{fig:ConvNetArchitecture}\n",
    "\\end{figure}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% RESULTS SECTION\n",
    "\\section{Results}\n",
    "\n",
    "\\subsection{Single Layer Net Results}\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/SingleModelLoss.png}}\n",
    "\\caption{Loss (Mean Squared Error) for the single layer net over 120 epochs}\n",
    "\\label{fig:SingleModelLoss}\n",
    "\\end{figure}\n",
    "\n",
    "Shown in Fig. \\ref{fig:SingleModelLoss} is the training and validation loss history over 120 epochs. At the end of 120 epochs, the model had decreased to a MSE loss of 0.012 for the training data and 0.019 for the validation data. We can also plainly see the overfitting that plagued both of our neural nets, regardless of the hyperparameter tuning we performed to try to remove it.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/SinglePredictedVAct.png}}\n",
    "\\caption{Predicted values generated by single layer net vs actual value}\n",
    "\\label{fig:SinglePredictedVAct}\n",
    "\\end{figure}\n",
    "\n",
    "Fig. \\ref{fig:SinglePredictedVAct} shows the values predicted by the single layer net on the 90 testing images verus their generated scores. We see a linear correlation between the two, which is what we hoped to observe. A perfect linear correlation would have meant that the model was guessing exactly the same as the human score.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/SingleDiffVAct.png}}\n",
    "\\caption{Difference between predicted values generated by single layer net and actual values vs actual value}\n",
    "\\label{fig:SingleDiffVAct}\n",
    "\\end{figure}\n",
    "\n",
    "Fig. \\ref{fig:SingleDiffVAct} shows the difference between the values predicted by the single layer net for the 90 test images and their human scores versus the human scores. From this plot, it is easy to see that the net had a tendency to overscore models with low human scores and underscore models with high human scores. We can also see that the most accurate predictions occured for models with lower human scores, which makes sense given the data's skew towards worse models. Over the full 90 image set, the net attained a mean difference of 0.093 between its predicted values and the human scores.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/singleWeights.png}}\n",
    "\\caption{Visualization of weights for the single layer net}\n",
    "\\label{fig:singleWeights}\n",
    "\\end{figure}\n",
    "\n",
    "The final weights for the single layer net are shown in Fig. \\ref{fig:singleWeights}. We see that the net is placing emphasis on the bridge forming between the two galaxies as well as on the tidal distortions around the edges. Interestingly, the net also thinks that the core of the left galaxy is extremely important. This may be due to the fact that all of our model images are positioned roughly the same way, and in every image there is a bright spot marking the center of a galaxy at roughly the hotspot location shown in Fig. \\ref{fig:singleWeights}.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/SinglePredictedBest.png}}\n",
    "\\caption{Model that was scored the highest by the single layer neural net}\n",
    "\\label{fig:SinglePredictedBest}\n",
    "\\end{figure}\n",
    "\n",
    "\n",
    "Shown in Fig. \\ref{fig:SinglePredictedBest} is the model that the single layer net scored the highest (0.846). Its real human score was 0.946, so this shows good agreement between the net's perception and human perception of what the best model is. We can also visually verify that this chosen model closely resembles the target image shown in Fig. \\ref{fig:TargetGalaxy}.\n",
    "\n",
    "\\subsection{Convolutional Neural Net Results}\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/ConvModelLoss.png}}\n",
    "\\caption{Layout of the convolutional neural net}\n",
    "\\label{fig:ConvModelLoss}\n",
    "\\end{figure}\n",
    "\n",
    "In Fig. \\ref{fig:ConvModelLoss} we see very similar learning behavior to the single layer net. Both display overfitting issues and, after 120 epochs, reach approximately the same MSE. For the convolutional net, the final MSE for the training data was 0.013 and 0.021 which is just slightly worse than the single layer net. Qualitatively, the only difference in learning behavior was the prescence of a slight plateau for the fist 10 or so epochs before the net actually began to learn quickly.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/ConvPredictedVAct.png}}\n",
    "\\caption{Predicted values generated by convolutional net vs actual value}\n",
    "\\label{fig:ConvPredictedVAct}\n",
    "\\end{figure}\n",
    "\n",
    "Like with the single layer net, Fig. \\ref{fig:ConvPredictedVAct} shows a linear correlation between the predicted values and human scores. However, as the human scores increase, we see a wider spread in predicted values when compared to the single layer net. \n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/ConvDiffVAct.png}}\n",
    "\\caption{Difference between predicted values generated by convolutional net and actual values vs actual value}\n",
    "\\label{fig:ConvDiffVAct}\n",
    "\\end{figure}\n",
    "\n",
    "Once again, Fig. \\ref{fig:ConvDiffVAct} shows that the convolutional net has a tendency to overscore worse models and underscore better models. However, when compared to the single layer net, we see a much tighter grouping of worse models (human scores $\\leq0.3$) that the model was able to accurately guess with a difference of $-0.1 \\leq$ Diff $\\leq 0.1$. For the 90 image testing set, the convoltional net attained a mean difference of 0.112, which is slightly worse than the single layer net. \n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/convWeight1.png}}\n",
    "\\caption{Visualization of weights for the first convolution layer}\n",
    "\\label{fig:convWeight1}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/convWeight2.png}}\n",
    "\\caption{Visualization of weights for the second convolution layer}\n",
    "\\label{fig:convWeight2}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/layer1activations.png}}\n",
    "\\caption{Visualization of network activations for the first convolution layer}\n",
    "\\label{fig:layer1activations}\n",
    "\\end{figure}\n",
    "\n",
    "Fig. \\ref{fig:convWeight1} and Fig. \\ref{fig:convWeight2} show the weights for the first and second convolution layers respectively. However, it is hard to glean any useful information from just these weights on their own. That is why we have shown the activations of units in the first layer in Fig. \\ref{fig:layer1activations}. We can see that for this specific activation, the net was processing a model that was very similar to the target image. We can see units that are picking up the major features: galactic cores, tidal distortions, and the bridge. We can also plainly see the units that are detecting the edges of the galaxies, as well as some units that aren't really being utilized.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/ConvPredictedBest.png}}\n",
    "\\caption{Model that was scored the highest by the convolutional neural net}\n",
    "\\label{fig:ConvPredictedBest}\n",
    "\\end{figure}\n",
    "\n",
    "Fig. \\ref{fig:ConvPredictedBest} shows the image that was given the highest score by the convolutional net (0.695). This predicted value is significantly lower than the human score of 0.853, but it shows that the  convolutional net was still atleast somewhat successful in deciding on what the best model should look like. Once again, we can visually verify that the convolutional net's predicted best model closely resembles the target image shown in Fig. \\ref{fig:TargetGalaxy}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% DISCUSSION SECTION\n",
    "\\section{Discussion}\n",
    "\n",
    "Both the single layer neural network and convolutional neural network show promise in correctly predicting how good of a fit a model is for a target galaxy collision. Both nets achieved a MSE of $\\approx 0.02$ on the validation data and a mean difference of $\\approx 0.1$ when tasked with predicting the scores of 90 test models. Both nets successfully chose an image out of a set of 90 that they decided was the best model for the target image. Upon visual comparison, these two chosen models very closely resemble the target galaxy collision. However, both models struggled with overfitting which can most likely be attributed to both our data set's small size and its uneven distribution of model quality. \n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=0.75\\linewidth]{./Images/ConvHumanNoiseExample.png}}\n",
    "\\caption{Example of a model that the convolutional net scored \"poorly\" when just comparing against human score.}\n",
    "\\label{fig:ConvHumanNoiseExample}\n",
    "\\end{figure}\n",
    "\n",
    "Another factor to consider is the noise associated with the human scores. As a citizen scientist project, the models in the Galaxy Zoo: Mergers data set were ranked by people with minimal training. There are also no checks in place to prevent against people clicking though models without consideration, artificially inflating or deflating their scores. We must also acknowledge the randomness of the tournament system. It is entirely possible that some models were \"unlucky\" and went against rounds of exclusively good models, causing them to be scored lower than they actually should have or vice versa.\n",
    "\n",
    "Take for example the model shown in Fig. \\ref{fig:ConvHumanNoiseExample}. This was one of the convolutional net's worst predictions, with a difference of nearly 0.5 between the net's predicted score of 0.195 and the human score of 0.676. However, if we visually compare this model with the target image, one could argue that the model did not deserve such a high human score. Instead, the model might better represent a score between the net's prediction and the human score. \n",
    "\n",
    "Future work could pertain to the expansion of the convolutional net to include more layers. Training with these extra layers and units would necessitate a larger data set, hopefully with a more even distribution of model quality. We believe that such a net would obtain better results than either of the nets described in this paper.  Also, the nets we created for this paper could be applied to the other 61 galaxy pairs, and, with enough training, possibly eliminate the need for further human rankings. This would allow for new models to be created and scored much faster than they have been previously. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% REFERENCES\n",
    "% THIS IS CREATED AUTOMATICALLY\n",
    "\\bibliographystyle{IEEEtran}\n",
    "\\bibliography{references} % change if another name is used for References file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\end{document}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
